---
title: 'Appendix B: Question 2 Code'
output:
  pdf_document: 
    fig_height: 5
    fig_width: 5
  html_notebook: default
editor_options: 
  chunk_output_type: console
---
This appendix contains all the code to answer question 2 of the assignment. It should be considered along with the report wherein the analisis can be found.

For each model created a confusion is generated to compute all relevent metrics like Accuracy, Sensitivity, Specificity, Kappa and the F1 Statistic. These values are stored within a dataframe called `modelPreformance` and are then used later in question 2.e to evaluate and compare the models generated. As each model is generated the associated confusion matrix is printed at the end of the computation.
```{r echo=T, results='hide'}
# Setup work space, install packages and import libs
rm(list = ls())
suppressMessages(library(caret))
suppressMessages(library(ROCR))
suppressMessages(library(blorr))
suppressMessages(library(magrittr))
suppressMessages(library(glmnet))
suppressMessages(library(MASS))
suppressMessages(library(caret))
suppressMessages(library(h2o))
suppressMessages(library(tictoc))
suppressMessages(library(heplots))
suppressMessages(library(knitr))
suppressMessages(library(klaR))
options(warn = -1)

# Extract the testing and training data from the provided csv files. These
# should be stored in the same root directory as this notebook.
Spam <- data.frame()

Spam.test <- read.csv("spam_test.csv")

full.data <- read.csv("spam_data.csv")
# we further split the training set up into a training and validation set so that
# hyperparameters can be chosen and models compared without biasing the results
# into the training set. For this, 90% of the data is used in the training set
# and 20% of the rows are left as the validation set.

set.seed(42)
#~80% of the set
train <- sample(seq_len(nrow(full.data)), size = ceiling(dim(full.data)[1]*0.8))
Spam.train <- full.data[train, ] #2881 observations used for training
Spam.validation <- full.data[-train, ] #720 observations left for validation

# Remove the index column from the data
Spam.train <- Spam.train[, -1]
Spam.test <- Spam.test[, -1]
Spam.validation <- Spam.validation[, -1]

train <- seq(1, nrow(Spam.train))

# Create a dataframe to store the results modelPreformance. This is populated
# Later after each model is fit
modelValidationPreformance <- data.frame()
modelTestPreformance <- data.frame()
```

First, a generic function needs to be created to store the preformance metrics for models. This function takes in `confusionMatrix` generated by `caret` and stores the relevent information to be used later on.

```{r}
storeModelValidationPreformance <- function(modelName, confusionMatrix){
  #takes the incoming confusion matrix and adds it to the dataframe. note the 
  #use of the <<- syntax notating that the function can modify the dataframe
  #modelValidationPreformance which is out of the scope of this function.
  modelValidationPreformance <<- rbind(
    modelValidationPreformance,
    data.frame(
      modelName = modelName,
      Accuracy = confusionMatrix$overall["Accuracy"],
      Kappa = confusionMatrix$overall["Kappa"],
      TruePositiveRate = confusionMatrix$byClass["Sensitivity"],
      TrueNegativeRate = confusionMatrix$byClass["Specificity"],
      FalsePositiveRate = 1 - confusionMatrix$byClass["Specificity"],
      FlaseNegativeRate = 1 - confusionMatrix$byClass["Sensitivity"],
      F1 = confusionMatrix$byClass["F1"]
    )
  )
}

storeModelTestPreformance <- function(modelName, confusionMatrix){
  modelTestPreformance <<- rbind(
    modelTestPreformance,
    data.frame(
      modelName = modelName,
      Accuracy = confusionMatrix$overall["Accuracy"],
      Kappa = confusionMatrix$overall["Kappa"],
      TruePositiveRate = confusionMatrix$byClass["Sensitivity"],
      TrueNegativeRate = confusionMatrix$byClass["Specificity"],
      FalsePositiveRate = 1 - confusionMatrix$byClass["Specificity"],
      FlaseNegativeRate = 1 - confusionMatrix$byClass["Sensitivity"],
      F1 = confusionMatrix$byClass["F1"]
    )
  )
}
```


\section{Question 1.a: Logistic Regression Model}
First we use standard logistic regression to fit a model onto the data set. To make the visulization of outputs easier the `confusionMatrix` function is used from the `caret` package. This is effectivly the same as calling `table` on both the predicted and actual results and then calculating the true positive and negative rates. The key terms from the confusion matrix of intrest are:
\begin{enumerate}
\item *Accuracy:* Quantifies the overall model quality of fit
\item *Kappa:* similar to Accuracy score, but it takes into account the accuracy that would have happened anyway through random predictions.
\item *Sensitivity:* true positive rate (correctly identified emails)
\item *Specificity:* true negative rate (correctly identified spam)
\item *F1:* = cosniders both the precision and recall to quantify quality such that$(2 \times Precision \times Recall) / (Precision + Recall)$
\end{enumerate}
\subsection{General Linear Model}
First step is to fit a general linear model, with famialy binomial, to indicate the fit is creating a standard logistic regression model.
```{r, warning=FALSE}
glm.fit <- glm(spam ~ .,
               data = Spam.train,
               family = binomial)

# train predictions
probs <- predict(glm.fit, newdata = Spam.train[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
confusionMatrix(factor(glm.pred), Spam.train$spam, mode = "everything")

# validation predictions
probs <- predict(glm.fit, newdata = Spam.validation[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
(cm <- confusionMatrix(factor(glm.pred), Spam.validation$spam, mode = "everything"))
storeModelValidationPreformance("GLM", cm)

plot(probs, as.factor(glm.pred),
     col=(Spam.validation$spam=="spam")+9, #colour 10 is red, colour 11 is green
     type="p",
     xlab="probability",
     ylab="Responce",
     yaxt="n")
grid()
abline(v=c(0.5), lty=2, lwd=3)
axis(2, at=1:2, labels=c('Not Spam','Spam'))

# Roc
glm.fit %>%
  blr_gains_table() %>%
  blr_roc_curve(xaxis_title = "False Positive Rate",
                yaxis = "True Positive Rate") +
  theme_bw()

# test predictions
probs <- predict(glm.fit, newdata = Spam.test[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
(cm <- confusionMatrix(factor(glm.pred), Spam.test$spam, mode = "everything"))
storeModelTestPreformance("GLM", cm)
```

\subsection{Forward, backward and bothways stepwise selection}
```{r, results="hide"}
#***backwards***#
suppressMessages(backwards <- stepAIC(glm.fit,
                  direction = "backward",
                  trace=FALSE))

#predict validation results
probs <- predict(backwards, newdata = Spam.validation[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store validation results
cm <- confusionMatrix(factor(glm.pred), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("GLM Backward", cm)

#test results (for question 2.e)
probs <- predict(backwards, newdata = Spam.test[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store test results
cm <- confusionMatrix(factor(glm.pred), Spam.test$spam, mode = "everything")
storeModelTestPreformance("GLM Backward", cm)
    

#***forwards***#
forwards <- step(glm.fit,
                  direction = "forward",
                  trace=FALSE)

#predict validation results
probs <- predict(forwards, newdata = Spam.validation[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store validation results
cm <- confusionMatrix(factor(glm.pred), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("GLM Forwards", cm)

#test results (for question 2.e)
probs <- predict(forwards, newdata = Spam.test[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store test results
cm <- confusionMatrix(factor(glm.pred), Spam.test$spam, mode = "everything")
storeModelTestPreformance("GLM Forwards", cm)

#***bothways***#
bothways <- step(glm.fit,
                  direction = "both",
                  trace=FALSE)
#predict validation results
probs <- predict(bothways, newdata = Spam.validation[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store validation results
cm <- confusionMatrix(factor(glm.pred), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("GLM Bothways", cm)

#test results (for question 2.e)
probs <- predict(bothways, newdata = Spam.test[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store test results
cm <- confusionMatrix(factor(glm.pred), Spam.test$spam, mode = "everything")
storeModelTestPreformance("GLM Bothways", cm)
```

\subsection{Cross Validation with Lasso on General Linear Model}
Next, we can try using diffrent variable selection methods, such as using cross validation with lasso to select the best posible model. This is has the effect of removing irrelevent variables from the model and in the process refines the output. The output of the Lasso is also plotted to show the effect of increasing lambda with the output.
```{r}
#generate lasso model
x <- model.matrix(spam ~ ., data = Spam.train)
cv.lasso <- cv.glmnet(x, Spam.train[, 58], alpha = 1, family = "binomial")

#validation results
newx <- model.matrix(spam ~ ., data = Spam.validation)
cv.probs <- predict(cv.lasso, newx = newx, type = "response", s = cv.lasso$lambda.min)
cv.pred <- rep("email", dim(cv.probs)[1])
cv.pred[cv.probs > 0.5] <- "spam"
cm <- confusionMatrix(factor(cv.pred), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("CV Lasso GLM", cm)

#test results
newx <- model.matrix(spam ~ ., data = Spam.test)
cv.probs <- predict(cv.lasso, newx = newx, type = "response", s = cv.lasso$lambda.min)
cv.pred <- rep("email", dim(cv.probs)[1])
cv.pred[cv.probs > 0.5] <- "spam"
cm <- confusionMatrix(factor(cv.pred), Spam.test$spam, mode = "everything")
storeModelTestPreformance("CV Lasso GLM", cm)

plot(cv.lasso)
cm
```
\subsection{Repeated cross validation with linear model}

Last thing to try in variable selection is to employ the `caret` package and try doing some kind of cross validated lasso using on the glm method. We can also look at variable importance using this method.
```{r, warning=FALSE}
controller <- trainControl(
  method = "repeatedcv", # repeated cross validation
  number = 10, # k = 10 folds
  repeats = 5, # CV is done 5 times with 5 diffrent sets of k splits
  verboseIter = F
)

linear <- train(spam ~ .,
  data = Spam.train,
  method = "glm",
  trControl = controller
)

# validation results
probs <- predict(linear$finalModel, newdata = Spam.validation, type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
cm <- confusionMatrix(factor(glm.pred), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Repeated CV GLM",cm)

plot(varImp(linear, scale = T))
cm

# Testing results
probs <- predict(linear$finalModel, newdata = Spam.test, type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
cm <- confusionMatrix(factor(glm.pred), Spam.test$spam, mode = "everything")
storeModelTestPreformance("Repeated CV GLM",cm)
```
Comparing diffrent variable selection techniques.
```{r}

rownames(modelValidationPreformance) <- c() # clean up the row data from the model preformance
modelValidationPreformance[order(modelValidationPreformance$Kappa, decreasing = TRUE),]
#Looking at number of variables included in the models:
  
coefs <- coef(cv.lasso, cv.lasso$lambda.min)
print(colSums(coefs!=0))

```
\section{Question 2.b: Discriminant Analysis}
Next discriminant analysis is applied to try and get a more refined model. Both linear discriminant analysis and quadratic discriminant analysis are applied and the same confusion matrix is generated.

\subsection{Linear discriminat analysis}
```{r}
lda.fit <- lda(spam ~ ., data = Spam.train)
plot(lda.fit)

#prediction results
lda.pred <- predict(lda.fit, newdata = Spam.validation, type = "response")
cm <- confusionMatrix(factor(lda.pred$class), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("LDA",cm)
cm

plot(lda.pred$x[,1], lda.pred$class, col=(Spam.validation$spam=="spam")+10, type="p")

#test results
lda.pred <- predict(lda.fit, newdata = Spam.test, type = "response")
cm <- confusionMatrix(factor(lda.pred$class), Spam.test$spam, mode = "everything")
storeModelTestPreformance("LDA",cm)
```

LDA stepwise selection. for this the greedy.wilks function is used to preform variable selection.
```{r}
#select the relevent variables
lda.wilks <- greedy.wilks(spam ~ .,data=Spam.train, niveau = 0.05)  

#build the model on the new variables
lda.fit <- lda(lda.wilks$formula, data=Spam.train, method="moment")

# Step2) Discriminant Score Estimates for Each Observation ####
pred <- predict(lda.fit)

# Plot the discriminant scores
plot(lda.fit, dimen = 1, type="both") 

plot(pred$x[,1],
     xlab="Observation Index",
     ylab="Discriminant Magnitude",
     col=factor(Spam.train$spam))
legend("topleft",
       legend=levels(factor(Spam.train$spam)), 
       text.col=seq_along(levels(factor(Spam.train$spam))))
grid()

#prediction results
lda.pred <- predict(lda.fit, newdata = Spam.validation, type = "response")
cm <- confusionMatrix(factor(lda.pred$class), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("LDA, wilks",cm)
cm

#test results
lda.pred <- predict(lda.fit, newdata = Spam.test, type = "response")
cm <- confusionMatrix(factor(lda.pred$class), Spam.test$spam, mode = "everything")
storeModelTestPreformance("LDA, wilks",cm)
```

\subsection{Quadratic discriminant analysis}
```{r}
qda.fit <- qda(spam ~ ., data = Spam.train)

#validation preformance
probs <- predict(qda.fit, newdata = Spam.validation, type = "response")
cm <- confusionMatrix(probs$class, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("QDA",cm)
cm

#validation preformance
probs <- predict(qda.fit, newdata = Spam.test, type = "response")
cm <- confusionMatrix(probs$class, Spam.test$spam, mode = "everything")
storeModelTestPreformance("QDA",cm)
```
\subsection{Assumption Testing}
LDA assumes normal distributed data, features that are statistically independent, and identical covariance matrices for every class. These assumptions are now tested. When these assumptions hold, then LDA approximates the Bayes classifier very closely and the discriminant function produces a linear decision boundary.
QDA assumes that observation of each class are drawn from a normal distribution (same as LDA). It also assumes that each class has its own covariance matrix (different from LDA).
Assumptions:

```{r}
### LDA Assumptions.
#1: normality testing. Doesent really make sence with such a large data set but
#will do it in any case. using the shapiro.test and Multivariate Normality will
#be tested using MVN.
test <- shapiro.test(Spam.train[,1])
results <- rep(0,dim(Spam.train)[2] - 1)

for (i in 1:length(results)){
  results[i] <- shapiro.test(Spam.train[,i])
}

library(MVN)
result <- mvn(data = Spam.validation[-58])
result$multivariateNormality

# Graphical Assessment of Multivariate Normality
x <- as.matrix(Spam.train[-58])
center <- colMeans(x)
n <- nrow(x); 
p <- ncol(x); 
cov <- cov(x); 
d <- mahalanobis(x,center,cov) # distances 
qqplot(qchisq(ppoints(n),df=p),d,
       main="QQ Plot Assessing Multivariate Normality",
       ylab="Mahalanobis D2")
abline(a=0,b=1)


#2: statistically independence
(chiSqaured <- chisq.test(x = Spam.train[-58],
                         y = Spam.train[58]))

#3: identical covariance matrices for every class
(res <- boxM(Spam.train[,-58], Spam.train$spam))

```

\section{Question 2.c}
Random forrest train a bunch of tree independently, using a random sample of the data. This randomness helps to make the model more robust than a single decision tree, and less likely to overfit on the training data. Values are then taken as an average from the generated set of trees. First a generic random forrest is created and then the `h2o.grid` function will be used to try various combinations of the different possible model parameters. 
```{r, echo=T, results='hide'}
suppressMessages(h2o.init(max_mem_size = "10g"))


invisible(
  rf.spam <- h2o.randomForest(
    training_frame = as.h2o(Spam.train),
    y = 58,
    seed = 1
  )
)

#validation results
rf.probs <- h2o.predict(
  object = rf.spam,
  newdata = as.h2o(Spam.validation)
)
```

```{r}
# we need to do a bit of manipulation to get the h2o dataframe out of it's given
#format to put it into a data frame that the confusionMatrix function from caret
#can understand alternativly, one can use# the confusion matrix function from
#h2o but this formats data diffrently so for the sake of consistancy I will
#format it this way. The h2o function would be: 
#h2o.confusionMatrix(rf.spam, as.h2o(Spam.test))
rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Random Forest",cm)
cm
```

```{r results='hide'}
#testing results
rf.probs <- h2o.predict(
  object = rf.spam,
  newdata = as.h2o(Spam.test)
)
```


```{r}
rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.test$spam, mode = "everything")
storeModelTestPreformance("Random Forest",cm)
cm
```

Next, the h2o.grid function is impelemented. This is done to build up a number of
diffrent models over a range of diffrent hyper parameters

```{r results='hide'}
#this will generate 125 diffrent models: for each  mtries, each ntrees and
#each max_dept. 
#Number of variables randomly sampled as candidates at each split.
mtries <- floor(seq(floor(sqrt(length(Spam.train))/2),
              floor(sqrt(length(Spam.train))*3),
              length.out = 5)) 
ntrees = floor(seq(50, 300,length = 5)) #Number of trees
max_depth = seq(10, 50,length = 5) #Maximum tree depth

hyper_params <- list(mtries = mtries, ntrees = ntrees, max_depth = max_depth)

rf.grid <- h2o.grid(algorithm = "randomForest",
                    hyper_params = hyper_params,
                    x = 1:57, y = 58, 
                    training_frame = as.h2o(Spam.train),
                    #validation_frame = datTest_h2o,
                    nfolds = 0,
                    seed = 1)


rf.best <- h2o.getModel(rf.grid@model_ids[[1]])

#Validation results
suppressMessages(
  rf.probs <- h2o.predict(
    object = rf.best,
    newdata = as.h2o(Spam.validation)
  )
)
```

```{r}
#calculate the results and store
rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Random Forest,Grid",cm)
cm
```

```{r results='hide'}
#Testing results
suppressMessages(
  rf.probs <- h2o.predict(
    object = rf.best,
    newdata = as.h2o(Spam.test)
  )
)
```

```{r}
rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.test$spam, mode = "everything")
storeModelTestPreformance("Random Forest,Grid",cm)
```

\subsection{generate a plot to show the OOB error}
```{r results='hide'}
models=list()
n <- 20 #the number of models to plot

for(i in 1:n){
  models[i] = h2o.getModel(rf.grid@model_ids[[i]])
}

# capture the requried x lables
xlabel = NULL
for(i in 1:n){
xlabel[i] = paste(models[[i]]@allparameters$ntrees,
                  models[[i]]@allparameters$mtries,
                  models[[i]]@allparameters$max_depth)
}

#capture the mse to quantify the OOB error
res_devOOB = NULL
for(i in 1:n){
res_devOOB[i] = h2o.mse(models[[i]])
}

#calculate the test set mse
res_devTest = NULL
for (i in 1:n) {
  res_devTest[i] <- h2o.mse(h2o.performance(h2o.getModel(rf.grid@model_ids[[i]]),
                                            newdata = as.h2o(Spam.test)))
}
```
Plot of training errors for different models
```{r}
#trying to see if the out of bag error and the test errors give similar results
plot(res_devTest[1:n],
     xaxt = "n",
     ylab='Residual Deviance',
     xlab='Grid Parameters (ntrees, mtries, max_depth)',
     ylim=c(min(res_devOOB,res_devTest)*0.95,
            max(res_devOOB,res_devTest)*1.05),
     type = "b",
     col ="red")
axis(1,
     at=1:n,
     labels=xlabel[1:n])
lines(res_devOOB[1:n],
      type = "o",
      col ="blue")
legend("topleft",
       c("OOB","Test"),
       pch = 21,
       pt.bg = "white",
       lty = 1,
       col = c("blue", "red"))
grid(col = "lightgray", lty = "dotted")
```

Variable importance plot
```{r results='hide'}
h2o.varimp_plot(rf.best)
```

Now that we know what the ideal values are for the random forest we can try and refine the parameters even further by tuning the range of the hyper parameters to prevent saturation.
```{r results='hide'}
#this will generate 125 diffrent models: for each  mtries, each ntrees and
#each max_dept. 
#Number of variables randomly sampled as candidates at each split.
mtries <- floor(seq(floor(sqrt(length(Spam.train))/2),
              floor(sqrt(length(Spam.train))*2),
              length.out = 5)) 
ntrees = floor(seq(250, 500,length = 5)) #Number of trees
max_depth = floor(seq(20, 50,length = 5)) #Maximum tree depth

hyper_params <- list(mtries = mtries, ntrees = ntrees, max_depth = max_depth)

rf.grid2 <- h2o.grid(algorithm = "randomForest",
                      hyper_params = hyper_params,
                      x = 1:57, y = 58, 
                      training_frame = as.h2o(Spam.train),
                      #validation_frame = datTest_h2o,
                      nfolds = 0,
                      seed = 1)

rf.best2 <- h2o.getModel(rf.grid2@model_ids[[1]])

#Validation results
rf.probs <- h2o.predict(
    object = rf.best2,
    newdata = as.h2o(Spam.validation)
  )
```

```{r}
rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Random Forest,Grid2",cm)
cm
```

```{r results='hide'}
#Testing results
rf.probs <- h2o.predict(
    object = rf.best2,
    newdata = as.h2o(Spam.test)
)
```

```{r}
rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.test$spam, mode = "everything")
storeModelTestPreformance("Random Forest,Grid2",cm)
```

\section{question 2.d: boosted trees}
Next, a classification tree is grown for boosted trees. Boosted trees build trees one at a time, where each new tree helps to correct errors made by previously trained tree. It is a more itterative process. As with random forests first a standard GBM is grown and then an itterative tree is created using `h2o.grid`.
```{r, echo=T, results='hide'}
gbm.spam <- h2o.gbm(training_frame = as.h2o(Spam.train),
                    x = 1:57,
                    y = 58,
                    seed = 1)
#validation results
gbm.probs <- h2o.predict(object = gbm.spam, newdata = as.h2o(Spam.validation))
gbm.probs <- as.data.frame(gbm.probs)
cm <- confusionMatrix(gbm.probs$predict, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Boosted Tree",cm)
cm

#testing results
gbm.probs <- h2o.predict(object = gbm.spam, newdata = as.h2o(Spam.test))

gbm.probs <- as.data.frame(gbm.probs)
cm <- confusionMatrix(gbm.probs$predict, Spam.test$spam, mode = "everything")
storeModelTestPreformance("Boosted Tree",cm)
```

```{r results='hide'}
#we not introduce learning_rate and learn_rate instead of mtries. still generating
# 125 diffrent values over the full grid
learn_rate = seq(0.005, 0.5, length = 5)
ntrees = seq(50, 250,length = 5) #Number of trees
max_depth = seq(10, 50,length = 5) #Maximum tree depth

hyper_params <- list(ntrees = ntrees, max_depth = max_depth, learn_rate = learn_rate)

gbm.grid <- h2o.grid(algorithm = "gbm",
                      hyper_params = hyper_params,
                      x = 1:57,
                      y = 58, 
                      training_frame = as.h2o(Spam.train),
                      nfolds = 0,
                      seed = 1)
gbm.best <- h2o.getModel(gbm.grid@model_ids[[1]])

#Validation results
gbm.probs <- h2o.predict(
  object = gbm.best,
  newdata = as.h2o(Spam.validation)
)
gbm.probs <- as.data.frame(gbm.probs)
```

```{r}
cm <- confusionMatrix(gbm.probs$predict, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Boosted Tree,Grid",cm)

```

```{r results='hide'}
#testing results
gbm.probs <- h2o.predict(
  object = gbm.best,
  newdata = as.h2o(Spam.test)
)
```

```{r}
gbm.probs <- as.data.frame(gbm.probs)

cm <- confusionMatrix(gbm.probs$predict, Spam.test$spam, mode = "everything")
storeModelTestPreformance("Boosted Tree,Grid",cm)

h2o.varimp_plot(gbm.best)
```

```{r results='hide'}
models=list()
n <- 20 #the number of models to plot

for(i in 1:n){
  models[i] = h2o.getModel(gbm.grid@model_ids[[i]])
}

# capture the requried x lables
xlabel = NULL
for(i in 1:n){
xlabel[i] = paste(models[[i]]@allparameters$ntrees,
                  models[[i]]@allparameters$learn_rate,
                  models[[i]]@allparameters$max_depth)
}

#capture the mse to quantify the OOB error
res_devOOB = NULL
for(i in 1:n){
res_devOOB[i] = h2o.mse(models[[i]])
}

#calculate the test set mse
res_devTest = NULL
for (i in 1:n) {
  res_devTest[i] <- h2o.mse(h2o.performance(h2o.getModel(gbm.grid@model_ids[[i]]),
                                            newdata = as.h2o(Spam.test)))
}
```

```{r}
# Plot of training errors for different models:
#trying to see if the out of bag error and the test errors give similar results
plot(res_devTest[1:n],
     xaxt = "n",
     xlab='Grid Parameters (ntrees, mtries, max_depth)',
     ylab='Residual Deviance',
     ylim=c(min(res_devOOB,res_devTest)*0.95,
            max(res_devOOB,res_devTest)*1.05),
     type = "b",
     col ="red")
axis(1,
     at=1:n,
     labels=xlabel[1:n])
lines(res_devOOB[1:n],
      type = "o",
      col ="blue")
legend("right",
       c("GBM Training","Training"),
       pch = 21,
       pt.bg = "white",
       lty = 1,
       col = c("blue", "red"))
grid(col = "lightgray", lty = "dotted")
```

\section{Question 2.e}

This question begings with selecting the best method based off accuracy overall from all testes preformed. We can then modify the ROC position if need be to inflence the Sensitivity spesificity ratio.

```{r}
rownames(modelTestPreformance) <- c() # clean up the row data from the model preformance
kable(modelTestPreformance[order(modelTestPreformance$Accuracy, decreasing = TRUE), ])
```

print the validation error results
```{r}
rownames(modelValidationPreformance) <- c() # clean up the row data from the model preformance
kable(modelValidationPreformance[order(modelValidationPreformance$Accuracy, decreasing = TRUE), ])
```

Some plots.
```{r}
# Fit statistics
blr_model_fit_stats(glm.fit)

# Roc
glm.fit %>%
  blr_gains_table() %>%
  blr_roc_curve(xaxis_title = "False Positive Rate", yaxis = "True Positive Rate")

# KS chart
glm.fit %>%
  blr_gains_table() %>%
  blr_ks_chart()

# lift
glm.fit %>%
  blr_gains_table() %>%
  plot()
```


create a glm in h2o so we can plot it's roc.
```{r results='hide'}
glm.h2o <- h2o.glm(x = 1:57,
                   y = 58, 
                   training_frame = as.h2o(Spam.train),
                   seed = 1234567,        # Seed for random numbers
                   family = "binomial",   # Outcome variable
                   lambda_search = FALSE,  # Optimum regularisation lambda
                   alpha = 0,           # Elastic net regularisation
                   nfolds = 1)             # N-fold cross validation

#Validation results
gbm.probs <- h2o.predict(
  object = glm.h2o,
  newdata = as.h2o(Spam.validation)
)
gbm.probs <- as.data.frame(gbm.probs)
```
```{r}
cm <- confusionMatrix(gbm.probs$predict, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Repeated CV, GLM",cm)
cm
```

this next graph's implementation was inspired from here: https://stackoverflow.com/questions/44034944/how-to-directly-plot-roc-of-h2o-model-object-in-r

```{r}


# for example I have 4 H2OModels
list(gbm.spam, rf.best2, glm.h2o) %>% 
  # map a function to each element in the list
  purrr::map(function(x) x %>% h2o.performance() %>% 
        # from all these 'paths' in the object
        .@metrics %>% .$thresholds_and_metric_scores %>% 
        # extracting true positive rate and false positive rate
        .[c('tpr','fpr')] %>% 
        # add (0,0) and (1,1) for the start and end point of ROC curve
        tibble::add_row(tpr=0,fpr=0,.before=T) %>% 
        tibble::add_row(tpr=0,fpr=0,.before=F)) %>% 
  # add a column of model name for future grouping in ggplot2
  purrr::map2(c('Best Gradient Boosted Tree','Best Random Forest','Best General Linear Model'),
        function(x,y) x %>% tibble::add_column(model=y)) %>% 
  # reduce four data.frame to one
  purrr::reduce(rbind) %>% 
  # plot fpr and tpr, map model to color as grouping
  ggplot(aes(fpr,tpr,col=model))+
  geom_line()+
  geom_segment(aes(x=0,y=0,xend = 1, yend = 1),linetype = 2,col='grey')+
  xlab('False Positive Rate')+
  ylab('True Positive Rate')+
  theme_bw() +
  theme(legend.position=c(0.8,0.2))
```

Lastly we will influence the threshold value to try and get the impact on the Error rate. First we will plot it for the GLM. Then it is done for the Boosted tree

```{r}
results <- data.frame()
probs <- predict(glm.fit, newdata = Spam.validation[-58], type = "response")

for (i in 0:100){
  pred <- rep("email", length(probs))
  pred[probs > i/100] <- "spam"
  (cm <- confusionMatrix(factor(pred), Spam.validation$spam, mode = "everything"))
  (results <- rbind(results,
                   data.frame(
                              index = i,
                              FalsePositiveRate = 1 - cm$byClass["Specificity"],
                              FlaseNegativeRate = 1 - cm$byClass["Sensitivity"],
                              OverallError = 1 - cm$overall["Accuracy"],
                              F1 = cm$byClass["F1"])))
                              
}

ggplot(results, aes(x=index, y=error)) + 
  geom_line(aes(y = FalsePositiveRate,
                colour = "False Positive Rate (Classifying legit email as spam)")) + 
  geom_line(aes(y = FlaseNegativeRate,
                colour = "False Negative Rate (Classifying spam as legit email)")) + 
  geom_line(aes(y = OverallError, colour = "Overall Error (Total missclasified)")) + 
  geom_line(aes(y = F1,
                colour = "F1 Statistic (normally maximized for ideal classifier)")) + 
  xlab('Threshold') +
  ylab('Error Rate') +
  labs(colour = "Error Statistic") +
  theme_bw() +
  theme(legend.position=c(0.5,0.5))

```

```{r}
probs <- predict(glm.fit, newdata = Spam.train[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.85] <- "spam"
confusionMatrix(factor(glm.pred), Spam.train$spam, mode = "everything")
```
Lets do the same process for the best gbm

```{r results='hide'}
probs <- h2o.predict(object = gbm.best, newdata = as.h2o(Spam.validation))
```

```{r}
results <- data.frame()

#we need to convert the h2o frame to a matrix to do the comparison
probs <- as.matrix(probs$spam)

#itterate through all elements and extract the values we want to plot
for (i in 0:100){
  pred <- rep("email", length(probs))
  pred[probs > i/100] <- "spam"
  (cm <- confusionMatrix(factor(pred), Spam.validation$spam, mode = "everything"))
  (results <- rbind(results,
                   data.frame(
                              index = i/100,
                              FalsePositiveRate = 1 - cm$byClass["Specificity"],
                              FlaseNegativeRate = 1 - cm$byClass["Sensitivity"],
                              OverallError = 1 - cm$overall["Accuracy"],
                              F1 = cm$byClass["F1"])))
}
#generate the plot 
ggplot(results, aes(index)) + 
  geom_line(aes(y = FalsePositiveRate,
                colour = "False Positive Rate (Classifying legit email as spam)")) + 
  geom_line(aes(y = FlaseNegativeRate,
                colour = "False Negative Rate (Classifying spam as legit email)")) + 
  geom_line(aes(y = OverallError, colour = "Overall Error (Total missclasified)")) + 
  geom_line(aes(y = F1,
                colour = "F1 Statistic (normally maximized for ideal classifier)")) + 
  geom_vline(xintercept=0.80, linetype = "dashed") +
  xlab('Threshold') +
  ylab('Error Rate') +
  labs(colour = "Error Statistic") +
  theme_bw() +
  theme(legend.position=c(0.5,0.5))
```


```{r results='hide'}
gbm.probs <- h2o.predict(object = gbm.best, newdata = as.h2o(Spam.test))
```
```{r}
gbm.probs <- as.data.frame(gbm.probs)

gbm.pred <- rep("email", length(gbm.probs$spam))
gbm.pred[gbm.probs$spam > 0.8] <- "spam"

cm <- confusionMatrix(as.factor(gbm.pred), Spam.test$spam, mode = "everything")
cm
```

