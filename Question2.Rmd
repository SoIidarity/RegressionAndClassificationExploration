---
title: 'Appendix B: Question 2 Code'
output:
  pdf_document: 
    fig_height: 5
    fig_width: 5
  html_notebook: default
editor_options: 
  chunk_output_type: inline
---
This appendix contains all the code to answer question 2 of the assignment. It should be considered along with the report wherein the analisis can be found.

For each model created a confusion is generated to compute all relevent metrics like Accuracy, Sensitivity, Specificity, Kappa and the F1 Statistic. These values are stored within a dataframe called `modelPreformance` and are then used later in question 2.e to evaluate and compare the models generated. As each model is generated the associated confusion matrix is printed at the end of the computation.
```{r echo=T, results='hide'}
# Setup work space, install packages and import libs
rm(list = ls())
suppressMessages(library(caret))
suppressMessages(library(ROCR))
suppressMessages(library(blorr))
suppressMessages(library(magrittr))
suppressMessages(library(glmnet))
suppressMessages(library(MASS))
suppressMessages(library(caret))
suppressMessages(library(h2o))
suppressMessages(library(tictoc))
suppressMessages(library(heplots))
options(warn = -1)

# Extract the testing and training data from the provided csv files. These
# should be stored in the same root directory as this notebook.
Spam <- data.frame()

Spam.test <- read.csv("spam_test.csv")

full.data <- read.csv("spam_data.csv")
# we further split the training set up into a training and validation set so that
# hyperparameters can be chosen and models compared without biasing the results
# into the training set. For this, 90% of the data is used in the training set
# and 20% of the rows are left as the validation set.

set.seed(42)
train <- sample(seq_len(nrow(full.data)), size = ceiling(dim(full.data)[1]*0.8)) #~80% of the set
Spam.train <- full.data[train, ] #2881 observations used for training
Spam.validation <- full.data[-train, ] #720 observations left for validation

# Remove the index column from the data
Spam.train <- Spam.train[, -1]
Spam.test <- Spam.test[, -1]
Spam.validation <- Spam.validation[, -1]

train <- seq(1, nrow(Spam.train))

# Create a dataframe to store the results modelPreformance. This is populated
# Later after each model is fit
modelValidationPreformance <- data.frame()
modelTestPreformance <- data.frame()
```

First, a generic function needs to be created to store the preformance metrics for models. This function takes in `confusionMatrix` generated by `caret` and stores the relevent information to be used later on.

```{r}
storeModelValidationPreformance <- function(modelName, confusionMatrix){
  #takes the incoming confusion matrix and adds it to the dataframe. note the 
  #use of the <<- syntax notating that the function can modify the dataframe
  #modelValidationPreformance which is out of the scope of this function.
  modelValidationPreformance <<- rbind(
    modelValidationPreformance,
    data.frame(
      modelName = modelName,
      Accuracy = confusionMatrix$overall["Accuracy"],
      Kappa = confusionMatrix$overall["Kappa"],
      TruePositiveRate = confusionMatrix$byClass["Sensitivity"],
      TrueNegativeRate = confusionMatrix$byClass["Specificity"],
      FalsePositiveRate = 1 - confusionMatrix$byClass["Specificity"],
      FlaseNegativeRate = 1 - confusionMatrix$byClass["Sensitivity"],
      F1 = confusionMatrix$byClass["F1"]
    )
  )
}

storeModelTestPreformance <- function(modelName, confusionMatrix){
  modelTestPreformance <<- rbind(
    modelTestPreformance,
    data.frame(
      modelName = modelName,
      Accuracy = confusionMatrix$overall["Accuracy"],
      Kappa = confusionMatrix$overall["Kappa"],
      TruePositiveRate = confusionMatrix$byClass["Sensitivity"],
      TrueNegativeRate = confusionMatrix$byClass["Specificity"],
      FalsePositiveRate = 1 - confusionMatrix$byClass["Specificity"],
      FlaseNegativeRate = 1 - confusionMatrix$byClass["Sensitivity"],
      F1 = confusionMatrix$byClass["F1"]
    )
  )
}
```


## Question 1.a: Logistic Regression Model
First we use standard logistic regression to fit a model onto the data set. To make the visulization of outputs easier the `confusionMatrix` function is used from the `caret` package. This is effectivly the same as calling `table` on both the predicted and actual results and then calculating the true positive and negative rates. The key terms from the confusion matrix of intrest are:
1. *Accuracy:* Quantifies the overall model quality of fit
2. *Kappa:* similar to Accuracy score, but it takes into account the accuracy that would have happened anyway through random predictions.
3. *Sensitivity:* true positive rate (correctly identified emails)
4. *Specificity:* true negative rate (correctly identified spam)
5. *F1:* = cosniders both the precision and recall to quantify quality such that$(2 \times Precision \times Recall) / (Precision + Recall)$

### General Linear Model
First step is to fit a general linear model, with famialy binomial, to indicate the fit is creating a standard logistic regression model.
```{r, warning=FALSE}
glm.fit <- glm(spam ~ .,
               data = Spam.train,
               family = binomial)

# train predictions
probs <- predict(glm.fit, newdata = Spam.train[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
confusionMatrix(factor(glm.pred), Spam.train$spam, mode = "everything")

# validation predictions
probs <- predict(glm.fit, newdata = Spam.validation[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
(cm <- confusionMatrix(factor(glm.pred), Spam.validation$spam, mode = "everything"))
storeModelValidationPreformance("GLM", cm)
    
#prediction vs accuraccy plot
plot(probs, as.factor(glm.pred), col=(Spam.validation$spam=="spam") + 10, type="p")

# Roc for glm
glm.fit %>%
  blr_gains_table() %>%
  blr_roc_curve(xaxis_title = "False Positive Rate", yaxis = "True Positive Rate")

# test predictions
probs <- predict(glm.fit, newdata = Spam.test[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
(cm <- confusionMatrix(factor(glm.pred), Spam.test$spam, mode = "everything"))
storeModelTestPreformance("GLM", cm)


```


### Forward, backward and bothways stepwise selection
```{r, results="hide"}
#***backwards***#
tic()
suppressMessages(backwards <- stepAIC(glm.fit,
                  direction = "backward",
                  trace=FALSE))
toc()

#predict validation results
probs <- predict(backwards, newdata = Spam.validation[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store validation results
cm <- confusionMatrix(factor(glm.pred), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("GLM Backward", cm)

#test results (for question 2.e)
probs <- predict(backwards, newdata = Spam.test[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store test results
cm <- confusionMatrix(factor(glm.pred), Spam.test$spam, mode = "everything")
storeModelTestPreformance("GLM Backward", cm)
    

#***forwards***#
tic()
forwards <- step(glm.fit,
                  direction = "forward",
                  trace=FALSE)
toc()

#predict validation results
probs <- predict(forwards, newdata = Spam.validation[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store validation results
cm <- confusionMatrix(factor(glm.pred), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("GLM Forwards", cm)

#test results (for question 2.e)
probs <- predict(forwards, newdata = Spam.test[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store test results
cm <- confusionMatrix(factor(glm.pred), Spam.test$spam, mode = "everything")
storeModelTestPreformance("GLM Forwards", cm)

#***bothways***#
tic()
bothways <- step(glm.fit,
                  direction = "both",
                  trace=FALSE)
toc()
#predict validation results
probs <- predict(bothways, newdata = Spam.validation[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store validation results
cm <- confusionMatrix(factor(glm.pred), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("GLM Bothways", cm)

#test results (for question 2.e)
probs <- predict(bothways, newdata = Spam.test[-58], type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
#store test results
cm <- confusionMatrix(factor(glm.pred), Spam.test$spam, mode = "everything")
storeModelTestPreformance("GLM Bothways", cm)
```

### Cross Validation with Lasso on General Linear Model
Next, we can try using diffrent variable selection methods, such as using cross validation with lasso to select the best posible model. This is has the effect of removing irrelevent variables from the model and in the process refines the output. The output of the Lasso is also plotted to show the effect of increasing lambda with the output.
```{r}
#generate lasso model
x <- model.matrix(spam ~ ., data = Spam.train)
cv.lasso <- cv.glmnet(x, Spam.train[, 58], alpha = 1, family = "binomial")

#validation results
newx <- model.matrix(spam ~ ., data = Spam.validation)
cv.probs <- predict(cv.lasso, newx = newx, type = "response", s = cv.lasso$lambda.min)
cv.pred <- rep("email", dim(cv.probs)[1])
cv.pred[cv.probs > 0.5] <- "spam"
cm <- confusionMatrix(factor(cv.pred), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("CV Lasso GLM", cm)

#test results
newx <- model.matrix(spam ~ ., data = Spam.test)
cv.probs <- predict(cv.lasso, newx = newx, type = "response", s = cv.lasso$lambda.min)
cv.pred <- rep("email", dim(cv.probs)[1])
cv.pred[cv.probs > 0.5] <- "spam"
cm <- confusionMatrix(factor(cv.pred), Spam.test$spam, mode = "everything")
storeModelTestPreformance("CV Lasso GLM", cm)

plot(cv.lasso)
cm
```

Next, consider looking at the prediction quality by a generating a ROC curve for the model. This can later be used to tune the model based on what outcome is most desired; true positive rate or false postive rate. In this context of this problem this will mean what is more important to the user? getting a spam in their main inbox or getting a non-spam in the spam directory? This important distinction is discussed in the report and explored later in question 2.e.
```{r}
pred <- prediction(cv.probs, Spam.test$spam)

# calculate probabilities for TPR/FPR for predictions
perf <- performance(pred, "tpr", "fpr")
# performance(pred,"auc") # shows calculated AUC for model
plot(perf, colorize = FALSE, col = "black") # plot ROC curve
lines(c(0, 1), c(0, 1), col = "gray", lty = 4)
```

### Repeated cross validation with linear model

Last thing to try in variable selection is to employ the `caret` package and try doing some kind of cross validated lasso using on the glm method. We can also look at variable importance using this method.
```{r, warning=FALSE}
controller <- trainControl(
  method = "repeatedcv", # repeated cross validation
  number = 10, # k = 10 folds
  repeats = 5, # CV is done 5 times with 5 diffrent sets of k splits
  verboseIter = F
)

linear <- train(spam ~ .,
  data = Spam.train,
  method = "glm",
  trControl = controller
)

# validation results
probs <- predict(linear$finalModel, newdata = Spam.validation, type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
cm <- confusionMatrix(factor(glm.pred), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Repeated CV GLM",cm)

plot(varImp(linear, scale = T))
cm

# Testing results
probs <- predict(linear$finalModel, newdata = Spam.test, type = "response")
glm.pred <- rep("email", length(probs))
glm.pred[probs > 0.5] <- "spam"
cm <- confusionMatrix(factor(glm.pred), Spam.test$spam, mode = "everything")
storeModelTestPreformance("Repeated CV GLM",cm)
```
Comparing diffrent variable selection techniques.
```{r}

rownames(modelValidationPreformance) <- c() # clean up the row data from the model preformance
modelValidationPreformance[order(modelValidationPreformance$Kappa, decreasing = TRUE),]
#Looking at number of variables included in the models:
  
coefs <- coef(cv.lasso, cv.lasso$lambda.min)
print(colSums(coefs!=0))

```


## Question 2.b: Discriminant Analysis
Next discriminant analysis is applied to try and get a more refined model. Both linear discriminant analysis and quadratic discriminant analysis are applied and the same confusion matrix is generated.

### Linear discriminat analysis
```{r}
lda.fit <- lda(spam ~ ., data = Spam.train)
plot(lda.fit, col=4)

#prediction results
lda.pred <- predict(lda.fit, newdata = Spam.validation, type = "response")
cm <- confusionMatrix(factor(lda.pred$class), Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("LDA",cm)
cm

plot(lda.pred$x[,1], lda.pred$class, col=(Spam.validation$spam=="spam") + 10, type="p")

#test results
lda.pred <- predict(lda.fit, newdata = Spam.test, type = "response")
cm <- confusionMatrix(factor(lda.pred$class), Spam.test$spam, mode = "everything")
storeModelTestPreformance("LDA",cm)
```




### Quadratic discriminant analysis
Trying out 
```{r}
qda.fit <- qda(spam ~ ., data = Spam.train)

#validation preformance
probs <- predict(qda.fit, newdata = Spam.validation, type = "response")
cm <- confusionMatrix(probs$class, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("QDA",cm)
cm

#validation preformance
probs <- predict(qda.fit, newdata = Spam.test, type = "response")
cm <- confusionMatrix(probs$class, Spam.test$spam, mode = "everything")
storeModelTestPreformance("QDA",cm)
```
Stepwise selection for LDA and QDA
```{r}
model <- greedy.wilks(spam ~ ., data = Spam.train, niveau = 0.05)

```

### Assumption Testing
LDA assumes normal distributed data, features that are statistically independent, and identical covariance matrices for every class. These assumptions are now tested. When these assumptions hold, then LDA approximates the Bayes classifier very closely and the discriminant function produces a linear decision boundary.
QDA assumes that observation of each class are drawn from a normal distribution (same as LDA). It also assumes that each class has its own covariance matrix (different from LDA).
Assumptions:

```{r}
### LDA Assumptions.


#1: normality testing. Doesent really make sence with such a large data set but
#will do it in any case. using the shapiro.test and Multivariate Normality will
#be tested using MVN.

test <- shapiro.test(Spam.train[,1])

results <- rep(0,dim(Spam.train)[2] - 1)

for (i in 1:length(results)){
  results[i] <- shapiro.test(Spam.train[,i])
}

library(MVN)
result <- mvn(data = Spam.train[-58])

#Graphical Assessment of Multivariate Normality
x <- as.matrix(Spam.train[-58])
center <- colMeans(x) # centroid
n <- nrow(x);
p <- ncol(x)
cov <- cov(x); 
d <- mahalanobis(x,center,cov)
qqplot(qchisq(ppoints(n),df=p),
       d,
       main="QQ Plot Assessing Multivariate Normality",
       ylab="Mahalanobis D2")
abline(a=0,b=1)
grid()

#2: statistically independence
(chiSqaured <- chisq.test(x = Spam.train[-58],
                         y = Spam.train[58]))

#3: identical covariance matrices for every class
(res <- boxM(Spam.train[,-58], Spam.train$spam))



#can look at the covariance matricies of all the rows...
correlationMatrix <- matrix(data = 0,
                           nrow = length(Spam.train) - 1,
                           ncol = length(Spam.train) - 1)

for (i in 1:length(Spam.train) - 1){
  for (j in 1:length(Spam.train) - 1){
    print(Spam.train[i,58] == Spam.train[j,58])
    if(Spam.train[i,58] == 'spam'){ #we dont want the main diagonal to have 1's on it
      correlationMatrix[i,j] <- cov(Spam.test[,i], Spam.test[,j])  
    }
  }
}

```


## Question 2.c
Random forrest train a bunch of tree independently, using a random sample of the data. This randomness helps to make the model more robust than a single decision tree, and less likely to overfit on the training data. Values are then taken as an average from the generated set of trees. First a generic random forrest is created and then the `h2o.grid` function will be used to try various combinations of the different possible model parameters. 
```{r, echo=T, results='hide'}
suppressMessages(h2o.init(max_mem_size = "10g"))


invisible(
  rf.spam <- h2o.randomForest(
    training_frame = as.h2o(Spam.train),
    y = 58,
    seed = 1
  )
)

#validation results
rf.probs <- h2o.predict(
  object = rf.spam,
  newdata = as.h2o(Spam.validation)
)

# we need to do a bit of manipulation to get the h2o dataframe out of it's given
#format to put it into a data frame that the confusionMatrix function from caret
#can understand alternativly, one can use# the confusion matrix function from
#h2o but this formats data diffrently so for the sake of consistancy I will
#format it this way. The h2o function would be: 
#h2o.confusionMatrix(rf.spam, as.h2o(Spam.test))
rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Random Forest",cm)
cm

#testing results
rf.probs <- h2o.predict(
  object = rf.spam,
  newdata = as.h2o(Spam.test)
)

rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.test$spam, mode = "everything")
storeModelTestPreformance("Random Forest",cm)
cm
```

Next, the h2o.grid function is impelemented. This is done to build up a number of
diffrent models over a range of diffrent hyper parameters

```{r}
#this will generate 125 diffrent models: for each  mtries, each ntrees and
#each max_dept. 
#Number of variables randomly sampled as candidates at each split.
mtries <- floor(seq(floor(sqrt(length(Spam.train))/2),
              floor(sqrt(length(Spam.train))*3),
              length.out = 5)) 
ntrees = floor(seq(50, 300,length = 5)) #Number of trees
max_depth = seq(10, 50,length = 5) #Maximum tree depth

hyper_params <- list(mtries = mtries, ntrees = ntrees, max_depth = max_depth)

rf.grid <- h2o.grid(algorithm = "randomForest",
                      hyper_params = hyper_params,
                      x = 1:57, y = 58, 
                      training_frame = as.h2o(Spam.train),
                      #validation_frame = datTest_h2o,
                      nfolds = 0,
                      seed = 1)

rf.best <- h2o.getModel(rf.grid@model_ids[[1]])

#Validation results
rf.probs <- h2o.predict(
  object = rf.best,
  newdata = as.h2o(Spam.validation)
)
rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Random Forest,Grid",cm)
cm

#Testing results
rf.probs <- h2o.predict(
  object = rf.best,
  newdata = as.h2o(Spam.test)
)
rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.test$spam, mode = "everything")
storeModelTestPreformance("Random Forest,Grid",cm)
```

#generate a plot to show the OOB error
```{r warning=TRUE}
models=list()
n <- 20 #the number of models to plot

for(i in 1:n){
  models[i] = h2o.getModel(rf.grid@model_ids[[i]])
}

# capture the requried x lables
xlabel = NULL
for(i in 1:n){
xlabel[i] = paste(models[[i]]@allparameters$ntrees,
                  models[[i]]@allparameters$mtries,
                  models[[i]]@allparameters$max_depth)
}

#capture the mse to quantify the OOB error
res_devOOB = NULL
for(i in 1:n){
res_devOOB[i] = h2o.mse(models[[i]])
}

#calculate the test set mse
res_devTest = NULL
for (i in 1:n) {
  res_devTest[i] <- h2o.mse(h2o.performance(h2o.getModel(rf.grid@model_ids[[i]]),
                                            newdata = as.h2o(Spam.test)))
}

# Plot of training errors for different models:
#trying to see if the out of bag error and the test errors give similar results
plot(res_devTest[1:n],
     xaxt = "n",
     ylab='Residual Deviance',
     xlab='Grid Parameters (ntrees, mtries, max_depth)',
     ylim=c(min(res_devOOB,res_devTest)*0.95,
            max(res_devOOB,res_devTest)*1.05),
     type = "b",
     col ="red")
axis(1,
     at=1:n,
     labels=xlabel[1:n])
lines(res_devOOB[1:n],
      type = "o",
      col ="blue")
legend("topleft",
       c("OOB","Test"),
       pch = 21,
       pt.bg = "white",
       lty = 1,
       col = c("blue", "red"))
grid(col = "lightgray", lty = "dotted")

```

```{r}

h2o.varimp_plot(rf.best)
```

#Now that we know what the ideal values are for the random forest we can try and refine the parameters even further by tuning the range of the hyper parameters to prevent saturation.
```{r}
#this will generate 125 diffrent models: for each  mtries, each ntrees and
#each max_dept. 
#Number of variables randomly sampled as candidates at each split.
mtries <- floor(seq(floor(sqrt(length(Spam.train))/2),
              floor(sqrt(length(Spam.train))*2),
              length.out = 5)) 
ntrees = floor(seq(250, 500,length = 5)) #Number of trees
max_depth = floor(seq(20, 50,length = 5)) #Maximum tree depth

hyper_params <- list(mtries = mtries, ntrees = ntrees, max_depth = max_depth)

rf.grid2 <- h2o.grid(algorithm = "randomForest",
                      hyper_params = hyper_params,
                      x = 1:57, y = 58, 
                      training_frame = as.h2o(Spam.train),
                      #validation_frame = datTest_h2o,
                      nfolds = 0,
                      seed = 1)

rf.best2 <- h2o.getModel(rf.grid2@model_ids[[1]])

#Validation results
rf.probs <- h2o.predict(
  object = rf.best2,
  newdata = as.h2o(Spam.validation)
)
rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Random Forest,Grid2",cm)
cm

#Testing results
rf.probs <- h2o.predict(
  object = rf.best2,
  newdata = as.h2o(Spam.test)
)
rf.probs <- as.data.frame(rf.probs)
cm <- confusionMatrix(rf.probs$predict, Spam.test$spam, mode = "everything")
storeModelTestPreformance("Random Forest,Grid2",cm)
```


## question 2.d: boosted trees
Next, a classification tree is grown for boosted trees. Boosted trees build trees one at a time, where each new tree helps to correct errors made by previously trained tree. It is a more itterative process. As with random forests first a standard GBM is grown and then an itterative tree is created using `h2o.grid`.
```{r, echo=T, results='hide'}
gbm.spam <- h2o.gbm(training_frame = as.h2o(Spam.train),
                    x = 1:57,
                    y = 58,
                    seed = 1)

#validation results
gbm.probs <- h2o.predict(object = gbm.spam, newdata = as.h2o(Spam.validation))
gbm.probs <- as.data.frame(gbm.probs)
cm <- confusionMatrix(gbm.probs$predict, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Boosted Tree",cm)
cm

#testing results
gbm.probs <- h2o.predict(object = gbm.spam, newdata = as.h2o(Spam.test))
gbm.probs <- as.data.frame(gbm.probs)
cm <- confusionMatrix(gbm.probs$predict, Spam.test$spam, mode = "everything")
storeModelTestPreformance("Boosted Tree",cm)
```



```{r}
#we not introduce learning_rate and learn_rate instead of mtries. still generating
# 125 diffrent values over the full grid
learn_rate = seq(0.005, 0.5, length = 5)
ntrees = seq(50, 250,length = 5) #Number of trees
max_depth = seq(10, 50,length = 5) #Maximum tree depth

hyper_params <- list(ntrees = ntrees, max_depth = max_depth, learn_rate = learn_rate)

gbm.grid <- h2o.grid(algorithm = "gbm",
                      hyper_params = hyper_params,
                      x = 1:57,
                      y = 58, 
                      training_frame = as.h2o(Spam.train),
                      nfolds = 0,
                      seed = 1)
gbm.best <- h2o.getModel(gbm.grid@model_ids[[1]])

#Validation results
gbm.probs <- h2o.predict(
  object = gbm.best,
  newdata = as.h2o(Spam.validation)
)
gbm.probs <- as.data.frame(gbm.probs)

cm <- confusionMatrix(gbm.probs$predict, Spam.validation$spam, mode = "everything")
storeModelValidationPreformance("Boosted Tree,Grid",cm)
cm

#testing results
gbm.probs <- h2o.predict(
  object = gbm.best,
  newdata = as.h2o(Spam.test)
)
gbm.probs <- as.data.frame(gbm.probs)

cm <- confusionMatrix(gbm.probs$predict, Spam.test$spam, mode = "everything")
storeModelTestPreformance("Boosted Tree,Grid",cm)

h2o.varimp_plot(gbm.best)
```
```{r}
models=list()
n <- 20 #the number of models to plot

for(i in 1:n){
  models[i] = h2o.getModel(gbm.grid@model_ids[[i]])
}

# capture the requried x lables
xlabel = NULL
for(i in 1:n){
xlabel[i] = paste(models[[i]]@allparameters$ntrees,
                  models[[i]]@allparameters$learn_rate,
                  models[[i]]@allparameters$max_depth)
}

#capture the mse to quantify the OOB error
res_devOOB = NULL
for(i in 1:n){
res_devOOB[i] = h2o.mse(models[[i]])
}

#calculate the test set mse
res_devTest = NULL
for (i in 1:n) {
  res_devTest[i] <- h2o.mse(h2o.performance(h2o.getModel(gbm.grid@model_ids[[i]]),
                                            newdata = as.h2o(Spam.test)))
}

# Plot of training errors for different models:
#trying to see if the out of bag error and the test errors give similar results
plot(res_devTest[1:n],
     xaxt = "n",
     xlab='Grid Parameters (ntrees, mtries, max_depth)',
     ylim=c(min(res_devOOB,res_devTest)*0.95,
            max(res_devOOB,res_devTest)*1.05),
     type = "b",
     col ="red")
axis(1,
     at=1:n,
     labels=xlabel[1:n])
lines(res_devOOB[1:n],
      type = "o",
      col ="blue")
legend("bottomright",
       c("OOB","Test"),
       pch = 21,
       pt.bg = "white",
       lty = 1,
       col = c("blue", "red"))
grid(col = "lightgray", lty = "dotted")

```

Question 2.e

This question begings with selecting the best method based off accuracy overall from all testes preformed. We can then modify the ROC position if need be to inflence the Sensitivity spesificity ratio.

```{r}
rownames(modelTestPreformance) <- c() # clean up the row data from the model preformance
modelTestPreformance[order(modelTestPreformance$Accuracy, decreasing = TRUE), ]
```

print the validation error results
```{r}
rownames(modelValidationPreformance) <- c() # clean up the row data from the model preformance
modelValidationPreformance[order(modelValidationPreformance$Accuracy, decreasing = TRUE), ]
```


```{r}
# Fit statistics
blr_model_fit_stats(glm.fit)

# Roc
gbm.spam %>%
  blr_gains_table() %>%
  blr_roc_curve(xaxis_title = "False Positive Rate", yaxis = "True Positive Rate")

# KS chart
glm.fit %>%
  blr_gains_table() %>%
  blr_ks_chart()

# lift
glm.fit %>%
  blr_gains_table() %>%
  plot()
```

```{r}
# for example I have 4 H2OModels
list(gbm.spam) %>% 
  # map a function to each element in the list
  map(function(x) x %>% h2o.performance(valid=T) %>% 
        # from all these 'paths' in the object
        .@metrics %>% .$thresholds_and_metric_scores %>% 
        # extracting true positive rate and false positive rate
        .[c('tpr','fpr')] %>% 
        # add (0,0) and (1,1) for the start and end point of ROC curve
        add_row(tpr=0,fpr=0,.before=T) %>% 
        add_row(tpr=0,fpr=0,.before=F)) %>% 
  # add a column of model name for future grouping in ggplot2
  map2(c('Logistic Regression','Decision Tree','Random Forest','Gradient Boosting'),
        function(x,y) x %>% add_column(model=y)) %>% 
  # reduce four data.frame to one
  reduce(rbind) %>% 
  # plot fpr and tpr, map model to color as grouping
  ggplot(aes(fpr,tpr,col=model))+
  geom_line()+
  geom_segment(aes(x=0,y=0,xend = 1, yend = 1),linetype = 2,col='grey')+
  xlab('False Positive Rate')+
  ylab('True Positive Rate')+
  ggtitle('ROC Curve for Four Models')
```


